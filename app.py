
import gradio as gr
from InstructorEmbedding import INSTRUCTOR
from embedding_utils import load_documents, embed_documents, create_vector_db, query_db

model = INSTRUCTOR("hkunlp/instructor-base")

documents = load_documents("vault")
texts, embeddings, metadatas = embed_documents(documents, model)
collection = create_vector_db(texts, embeddings, metadatas)

def ask_question(question):
    if not question.strip():
        return "‚ùóÔ∏è Veuillez poser une question."
    results = query_db(collection, model, question)
    if not results["documents"]:
        return "ü§∑ Aucun r√©sultat trouv√©."

    context = "\n\n".join(results["documents"][0])
    return f"üìö **Contexte extrait** :\n{context}\n\nü§ñ **R√©ponse hypoth√©tique** :\n{question}... (√† compl√©ter avec un LLM si besoin)"

iface = gr.Interface(
    fn=ask_question,
    inputs=gr.Textbox(placeholder="Pose ta question ici..."),
    outputs="markdown",
    title="Cocoon AI ‚Äì Question sur ton contenu",
    description="Pose une question √† partir de tes fichiers Obsidian (`vault/`) ‚Äì l'IA cherchera et r√©pondra avec le contexte."
)

iface.launch()
