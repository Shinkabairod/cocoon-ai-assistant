import gradio as gr
from sentence_transformers import SentenceTransformer
from embedding_utils import load_documents, embed_documents, create_vector_db, query_db

# âœ… Chargement du modÃ¨le stable
model = SentenceTransformer("all-MiniLM-L6-v2")

# âœ… Chargement des documents depuis /vault
documents = load_documents("vault")
texts, embeddings, metadatas = embed_documents(documents, model)
collection = create_vector_db(texts, embeddings, metadatas)

# âœ… Fonction principale
def ask_question(question):
    if not question.strip():
        return "â—ï¸ Veuillez poser une question."
    results = query_db(collection, model, question)
    if not results["documents"]:
        return "ğŸ¤· Aucun rÃ©sultat trouvÃ©."

    context = "\n\n".join(results["documents"][0])
    return f"ğŸ“š **Contexte extrait** :\n{context}\n\nğŸ¤– **RÃ©ponse hypothÃ©tique** :\n{question}... (Ã  complÃ©ter avec un LLM si besoin)"

# âœ… Interface Gradio
iface = gr.Interface(
    fn=ask_question,
    inputs=gr.Textbox(placeholder="Pose ta question ici..."),
    outputs="markdown",
    title="Cocoon AI â€“ Question sur ton contenu",
    description="Pose une question Ã  partir de tes fichiers Obsidian (`vault/`) â€“ l'IA cherchera et rÃ©pondra avec le contexte."
)

iface.launch()