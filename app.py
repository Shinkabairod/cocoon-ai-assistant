USER_ID = "user_001"
VAULT_PATH = f"vaults/{USER_ID}"
documents = load_documents(VAULT_PATH)
import gradio as gr
from sentence_transformers import SentenceTransformer
from embedding_utils import load_documents, embed_documents, create_vector_db, query_db

# ‚úÖ Chargement du mod√®le stable
model = SentenceTransformer("all-MiniLM-L6-v2")

# ‚úÖ Chargement des documents depuis /vault
documents = load_documents("vault")
texts, embeddings, metadatas = embed_documents(documents, model)
collection = create_vector_db(texts, embeddings, metadatas)

# ‚úÖ Fonction principale
def ask_question(question):
    if not question.strip():
        return "‚ùóÔ∏è Veuillez poser une question."
    results = query_db(collection, model, question)
    if not results["documents"]:
        return "ü§∑ Aucun r√©sultat trouv√©."

    context = "\n\n".join(results["documents"][0])
    return f"üìö **Contexte extrait** :\n{context}\n\nü§ñ **R√©ponse hypoth√©tique** :\n{question}... (√† compl√©ter avec un LLM si besoin)"

# ‚úÖ Interface Gradio
iface = gr.Interface(
    fn=ask_question,
    inputs=gr.Textbox(placeholder="Pose ta question ici..."),
    outputs="markdown",
    title="Cocoon AI ‚Äì Question sur ton contenu",
    description="Pose une question √† partir de tes fichiers Obsidian (`vault/`) ‚Äì l'IA cherchera et r√©pondra avec le contexte."
)

iface.launch()